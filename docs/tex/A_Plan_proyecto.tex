\apendice{Plan de Proyecto Software}

\section{Introducción}
En este apartado de la documentación de los anexos se pretende exponer de manera cronológica como ha ido evolucionando el desarrollo del estudio realizado.

Durante la maduración del mismo se ha empleado la metodología \textit{Scrum} para dividir cada fase del proyecto en \textit{sprints}, dentro de cada cuál se incluye una planificación. un desarrollo y una revisión en forma de reunión del mismo.

Además de todo lo mencionado anteriormente, también se va a estudiar la viabilidad del proyecto, tanto económica, como legal.

\paragraph{}
\paragraph{}
\paragraph{}
\paragraph{}
\paragraph{}


\section{Planificación temporal}

\subsection{Sprint 1 (15/02 - 17/03)}
\subsubsection{Planificación}
Tras una primera toma de contacto entre los miembros del estudio, se acordó que inicialmente se comenzaría con un trabajo de investigación sobre el funcionamiento de cada componente de un sistema basado en ELK, así como la instalación de los mismos y prueba de que se ejecutaban correctamente.

También se encomendó la tarea de crear el repositorio GitHub del estudio, en el que se irían actualizando a medida que se avanzaba las \textit{issues} del proyecto.

\subsubsection{Desarrollo}
En este primer mes se investigó a través de las fuentes principales de información de cada programa su instalación y uso básico de cara a probar con pruebas lo que ofrecían. En primer lugar ElasticSearch no opuso grandes dificultades, puesto que posee una extensa documentación, y su ejecución fue relativamente sencilla, cosa que no ocurrió con Logstash, ya que su comprensión era más compleja y requería de conocimientos más elevados que para las otros dos componentes. Por último en Kibana fue en el que más profundizo en este sprint, puesto que era el programa más intuitivo que más posibilidades y complementos ofrecía.

También se hizo énfasis en analizar programas alternativos a estos comparando las ventajas e inconvenientes frente a Elastic, Logstash y Kibana.

\subsubsection{Revisión}
En la segunda reunión se compartieron los resultados encontrados y por parte del tutor el programa que más interés desperto fue Logstash, puesto que las opciones que ofrecía erán las más interesantes de estudiar, cimentando así las bases del segundo sprint que comenzaría inmediatamente después.

\subsection{Sprint 2 (18/03 - 03/04)}
\subsubsection{Planificación}
En esta segunda fase del proyecto, una vez conocidas las características básicas de los componentes y como se relacionan entre sí, se creyó conveniente profundizar en las posibilidades de filtrado que ofrece Logstash, sabiendo diferenciar que se puede y que no se puede hacer.

También se empezó a mencionar la idea de aplicar tanto Machine Learning como MapReduce en algún momento del proceso a los datos, por lo que se encomendó el comienzo de la investigación de ambos temas.

\subsubsection{Desarrollo}
Por parte del tutor, se ofreció una fuente de información para poder aplicar funciones de Machine Learning de manera gratuita a fuentes de datos, \textit{Scikit-Learn}, de manera que durante este tiempo el estudio se enfocó en comprender el funcionamiento de esta librería, y de que manera la podíamos acoplar en el sistema ELK.

Por otro lado, indagar en las funciones avanzadas de Logstash, fue costoso, puesto que la información presente en internet era escasa y la mayoría antigua, por lo que las pruebas con este programa fueron un claro ensayo y error hasta dar con la tecla. Se profundizo en las operaciones de filtrado, agrupamientos básicos y discretizaciones en el apartado \textit{filter} de el archivo de configuración.

\subsubsection{Revisión}
En la tercera y cuarta reunión entre los miembros se discutió sobre que filtros erán más interesantes de aplicar, el orden que debían seguir los datos a lo largo de todo el proceso ETL, y como se podían integrar los resultados obtenidos por las funciones de \textit{Scikit-Learn} en Elastic para su posterior exposición en Kibana. Concluyendo así el segundo sprint del proyecto y abriendo temas para la tercera fase.

\paragraph{}
\paragraph{}
\paragraph{}


\subsection{Sprint 3 (04/04  - 20/04)}
\subsubsection{Planificación}
Y así entramos en la tercera etapa del estudio, en la que se plantearon diferentes tareas a realizar, como la carga con éxito de los datos del script del Iris en Kibana, continuar explorando los filtros de Logstash, y abordar el tema los \textit{streamings} de datos en vivos, que aún quedaba verde en este momento.

Otro punto a destacar como importante de cara a realizar en esta fase, es el de comparar donde era más eficiente y útil hacer MapReduce, sin en Logstash o en Kibana.

\subsubsection{Desarrollo}
Por un lado, gracias a los foros presentes en las páginas oficiales tanto de Elastic como de \textit{Scikit-Learn}, se consiguió combinar los dos, de manera que el tráfico de información entre ambos fuera fluido y exitoso. Esto se hizo tras realizarle unas modificaciones al script tanto en la forma en la que ingestaba los datos, como en la de como se exportaban los mismos, y hacia donde.

El tema de los \textit{data streams} siguió sin avanzar, puesto que no se encontraba de que manera se podía implementar esta forma de ingesta de datos en el sistema ELK. 

Y trás darle unas cuantas vueltas, se llegó a la conclusión de que lo más eficiente era realizar el MapReduce en Logstash con su función \textit{aggregate}, ya que ofrecía mejor rendimiento procesando los datos que haciéndolo directamente en Kibana.

\subsubsection{Revisión}
Así se zanjó lo relacionado con \textit{Scikit-Learn} de manera correcta y cubriendo la tarea encomendada, mientras que los \textit{streamings} de datos en vivo seguía en el aire y sin saber muy bien hacia donde seguir tirando.

\paragraph{}
\paragraph{}
\paragraph{}


\subsection{Sprint 4 (22/04 - 30/04)}
\subsubsection{Planificación}
Para este cuarto sprint, se vió que el tema de los \textit{data streams} había que finiquitarlo, por lo que se le dió máxima relevancia a este, de manera que se pudieran mandar datos en tiempo real a Elastic de manera que se les aplicara un filtrado con Logstash y pudieran ser expuestos en Kibana.

\subsubsection{Desarrollo}
Esto se logró gracias a las herramientas \textit{WebSocket}, la cuál permitio poder conectarnos a una fuente de datos, \textit{Finnhub}, que mandara los datos tanto a Elastic como a Logstash, y esos datos fueran depurados para ser mostrados en un \textit{dashboard} de Kibana. Esto se consiguió gracias a un \href{https://github.com/ColinEberhardt/awesome-public-streaming-datasets}{repositorio} público de GitHub que ofrecía diferentes maneras de interactuar con datos en vivo.

De manera paralela a los \textit{WebSockets}, se trabajó con una herramienta del ecosistemas Elastic llamada \textit{Filebeat}, la cuál ofrecía cubir la necesidad de poder mandar datos en \textit{streaming} hacia Elastic o Logstash, pero al ver que la alternativa encontrada era más eficiente y ofrecía más posibilidades, se optó por los \textit{WebSockets}.

\subsubsection{Revisión}
En la sexta reunión de los miembros se mostró tranquilidad al ver que gran parte de los problemas presentes habían sido solventados y que se comprendía de que manera se podían implementar \textit{data streams} en el sistema ELK.

Con esto, se empezó a plantear la posibilidad de empezar a preparar los escenarios finales junto con la documentación de el funcionamiento de cada uno.

\paragraph{}
\paragraph{}
\paragraph{}
\paragraph{}


\subsection{Sprint 5 (31/04 - 06/05)}
\subsubsection{Planificación}
Por lo que en la séptima reunión se hizo retrospectiva de lo que se tenía hasta el momento, y se decidió que el tema de las transformaciones de datos en Logstash aún no estaba en un buen punto, por lo que en este sprint se centró el tiempo en cerrar todo esto.

También se dió enfásis a la idea del \textit{Edge Computing}, y como podía ser relevante aplicarlo en el estudio.

\subsubsection{Desarrollo}
Finalmente se terminó de cerrar todo lo relacionado con los filtrados en Logstash al aplicar distintas funciones a una fuente de datos. Funciones como:
\begin{itemize}
    \item Borrar campos vacíos
    \item Transformación de tipos
    \item Operaciones con distintos campos
\end{itemize}

Por otra parte, hubo un trabajo de investigación sobre \textit{Edge Computing} de cara a poder documentarlo en la memoria, y se comprendió la idea que se le asociaba.

\subsubsection{Revisión}
Así se concluyó esta quinta fase, dando por sentados todos los escenarios que iban a estar presentes en la documentación técnica, y se comenzó a plantear la idea de la preparación e ilustración de los mismos.

\paragraph{}
\paragraph{}
\paragraph{}


\subsection{Sprint 6 (07/05 - 16/05)}
\subsubsection{Planificación}
Llegando a esta sexta etapa ya se empezaba a divisar el final en el horizonte y se encomendó la tarea de ilustrar y preparar minuciosamente cada escenario tratado de manera que la documentación de los mismos fuera más sencilla.

\subsubsection{Desarrollo}
Durante este tiempo se trabajó en preparar todos los directorios con la información relevante de cada escenario, así como ilustrar los \textit{dashboards} finales obtenidos en cada uno.

Se empezó a documentar en \textit{Overleaf}  la memoria del estudio de manera básica para ir teniendo un punto de partida.

\subsubsection{Revisión}
Tras la novena reunión de miembros se empezaron a realizar correcciones a la memoria y reestructuraciones a los escenarios de manera que las ideas presentadas fueran más claras y concisas.

\subsection{Sprint 7 (17/05 - 10/06)}
\subsubsection{Planificación}
En esta séptima etapa se continuó con el plan pensado para la anterior de seguir documentando los escenarios y los diferentes apartados de la memoria técina del estudio.

\subsubsection{Desarrollo}
Por lo que todo este tiempo se dedicó a realizar modificaciones y continuar agregando información a la memoria corrigiendo la estructura y los contenidos de cada apartado de manera que fuerán más acordes a un trabajo científico.

\subsubsection{Revisión}
Siguiendo la pauta de la anterior reunión las reuniones octava y novena estuvieron enfocadas a seguir mejorando el trabajo realizado en al documentación.

\subsection{Sprint 8 (11/06 - 10/07)}
\subsubsection{Planificación}
Tras una serie de revisiones tanto de la documentación de los anexos como de la memoria, se han planteado diversas mejoras en ambos por parte del tutor. También se ha mencionado el modificar el script en el que se aplica Machine Learning, de manera que los datos se carguen desde Elastic y una vez procesados sean devueltos. Para terminar se encomendó la tarea de realizar videos explicativos del proyecto, el pulido final del repositorio y la creación de una máquina virtual que contenga todo lo relacionado con el proyecto.

\subsubsection{Desarrollo}
Se dedicó todo el último mes a pulir detalles en la documentación de manera satisfactoria mediante una serie de revisiones. Se consiguió modificar la fuente de ingesta del script de Machine Learning con éxito, y se trabajó en todo lo mencionado para que la entrega del proyecto se pudiera hacer en fecha.

\subsubsection{Revisión}
Tras una última revisión de los contenidos, ambas partes quedaron satisfechas con el resultado y se acordó que esa era el producto final a entregar.

\section{Estudio de viabilidad}

\subsection{Viabilidad económica}
A lo largo de este estudio se ha empleado una versión gratuita de Elastic, en la cuál, por ejemplo, el plugin que habilita la función Machine Learning, está bloqueado. Por lo que Elastic ofrece esta y más funciones en sus diferentes niveles de suscripción:

\begin{enumerate}
    \item \textbf{Basic (Gratis)}
\begin{itemize}
        \item Ofrece un amplio mercado de plugins y de características limitadas, entre las cuales no se encuentra la de aplicar Machine Learning a los índices de datos.
    \end{itemize}

    \item \textbf{Gold (109 doláres al mes)}
    \begin{itemize}
        \item Ofrece características adicionales de seguridad, monitoreo de datos y algunas funciones de Machine Learning.
    \end{itemize}
    \item \textbf{Platinum (125 doláres al mes)}
    \begin{itemize}
        \item Ofrece todas las características del nivel Gold, además de acceso completo a las capacidades de Machine Learning, alertas avanzadas, y soporte premium.
    \end{itemize}
    \item \textbf{Enterprise (175 dólares al mes)}
    \begin{itemize}
        \item Ofrece todas las características del nivel Platinum, con funcionalidades adicionales de administración de los datos y un mejor soporte.
    \end{itemize}
\end{enumerate}

\subsection{Viabilidad legal}
Un sistema ELK usa principalmente dos tipos de licencias: 
\begin{itemize}
    \item La Licencia de Apache 2.0 para versiones anteriores a la 7.11.
    \item La Licencia Elastic para versiones a partir de la 7.11. Esta también ofrece licencias básicas gratuitas y comerciales para acceso a funciones avanzadas y soporte técnico.
\end{itemize}

